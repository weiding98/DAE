{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fp30SB4bxeQb"
   },
   "source": [
    "# Incremental RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OehMMKamyfnU"
   },
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nl2nREINDLiw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "# import gym\n",
    "import random\n",
    "# from scipy.interpolate import griddata\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVq0NM4QyfnV"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5paWqo7tWL2"
   },
   "source": [
    "## Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrrwGLzoyfnW"
   },
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Factorised Gaussian NoisyNet\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, sigma0=0.25):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.noisy_weight = nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features))\n",
    "        self.noisy_bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.noise_std = sigma0 / math.sqrt(self.in_features)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.register_noise()\n",
    "\n",
    "    def register_noise(self):\n",
    "        in_noise = torch.FloatTensor(self.in_features)\n",
    "        out_noise = torch.FloatTensor(self.out_features)\n",
    "        noise = torch.FloatTensor(self.out_features, self.in_features)\n",
    "        self.register_buffer('in_noise', in_noise)\n",
    "        self.register_buffer('out_noise', out_noise)\n",
    "        self.register_buffer('noise', noise)\n",
    "\n",
    "    def sample_noise(self):\n",
    "        self.in_noise.normal_(0, self.noise_std)\n",
    "        self.out_noise.normal_(0, self.noise_std)\n",
    "        self.noise = torch.mm(\n",
    "            self.out_noise.view(-1, 1), self.in_noise.view(1, -1))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.noisy_weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.noisy_bias.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def reset_noise_stream(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.noisy_weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.noisy_bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Note: noise will be updated if x is not volatile\n",
    "        \"\"\"\n",
    "        normal_y = nn.functional.linear(x, self.weight, self.bias)\n",
    "        if self.training:\n",
    "            # update the noise once per update\n",
    "            self.sample_noise()\n",
    "\n",
    "        noisy_weight = self.noisy_weight * self.noise\n",
    "        noisy_bias = self.noisy_bias * self.out_noise\n",
    "        noisy_y = nn.functional.linear(x, noisy_weight, noisy_bias)\n",
    "        return noisy_y + normal_y\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8tdmeD-tZew"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=1, window=4, noisy=False):\n",
    "        # dim: dimension of Expanding World\n",
    "        # window: the number of timesteps the agent can review\n",
    "        # noisy: toggle noisy nets\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window = window\n",
    "        self.noisy = noisy\n",
    "        \n",
    "        # we double the input size for the information of \"reward claimed\". \n",
    "        # 0 for reward claimed (no more reward on that dimension), 1 otherwise\n",
    "        input_size = dim*2*window \n",
    "        output_size = dim*2 + 1    # Additional action for NOOP\n",
    "        \n",
    "        # Feedforward policy of 2 hidden layers\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(input_size, 160),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(160, 160),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(160, output_size)\n",
    "        ) if not noisy else nn.Sequential(\n",
    "            NoisyLinear(input_size, 160),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(160, 160),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(160, output_size)\n",
    "        )\n",
    "        \n",
    "        #\\Phi for Relative Frequency\n",
    "        self.explorer = nn.Sequential(\n",
    "            nn.Linear(input_size, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, output_size)\n",
    "        )\n",
    "        #\\Psi for epsilon_t\n",
    "        self.meta_policy = nn.Sequential(\n",
    "            nn.Linear(input_size, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 2)\n",
    "        )\n",
    "        \n",
    "        #For RND\n",
    "        self.target = nn.Sequential(\n",
    "            nn.Linear(input_size, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40)\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(input_size, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 40)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.policy(state)\n",
    "    \n",
    "    def novelty(self, state):\n",
    "        return self.explorer(state)\n",
    "    \n",
    "    def tradeoff(self, state):\n",
    "        return self.meta_policy(state)\n",
    "    \n",
    "    def intrinsic(self, state):\n",
    "        t = self.target(state)\n",
    "        p = self.predictor(state)\n",
    "        return nn.MSELoss()(t, p)\n",
    "    \n",
    "    def add_dim(self):\n",
    "        self.dim += 1\n",
    "        \n",
    "        self.add_input(self.policy, noisy = self.noisy)\n",
    "        self.add_output(self.policy, noisy = self.noisy)\n",
    "        \n",
    "        self.add_input(self.explorer)\n",
    "        self.add_output(self.explorer, small_bias=True)\n",
    "        \n",
    "        self.add_input(self.meta_policy)\n",
    "#         self.meta_policy = nn.Sequential(\n",
    "#             nn.Linear(self.dim*2*self.window, 40),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(40, 2)\n",
    "#         )\n",
    "        \n",
    "        self.add_input(self.target)\n",
    "        self.add_input(self.predictor)\n",
    "    \n",
    "    def add_input(self, layers, noisy=False):\n",
    "        '''\n",
    "        Add an input dimension = 1(dimension)*2(position and remaining rewards)*4(window size) = 8 neurons\n",
    "        '''\n",
    "        \n",
    "        # we're going to deal with the first layer\n",
    "        index = 0\n",
    "        \n",
    "        # take a copy of the current weights stored in self._fc\n",
    "        old_weight = layers[index].weight.data\n",
    "        old_bias   = layers[index].bias.data\n",
    "        device     = old_weight.device\n",
    "        \n",
    "        # randomly initialize a tensor with the size of the wanted layer \n",
    "        new_weight = torch.zeros([old_weight.shape[0], 2*self.window]).to(device)\n",
    "        bound = 1 / math.sqrt(old_weight.shape[1]+2*self.window)\n",
    "        nn.init.uniform_(new_weight, -bound, bound)\n",
    "        \n",
    "        \n",
    "        # concatenate the old weights with the new weights\n",
    "        new_wi = torch.cat([old_weight, new_weight], dim=1)\n",
    "        \n",
    "        # reset weight and grad variables to new size\n",
    "        if not noisy:\n",
    "            layers[index] = nn.Linear(old_weight.shape[1]+2*self.window, old_weight.shape[0]).to(device)\n",
    "        else:\n",
    "            layers[index] = NoisyLinear(old_weight.shape[1]+2*self.window, old_weight.shape[0]).to(device)\n",
    "\n",
    "        # set the weight data to new values\n",
    "        layers[index].weight = torch.nn.Parameter(new_wi).to(device)\n",
    "        layers[index].bias   = torch.nn.Parameter(old_bias).to(device)\n",
    "        \n",
    "    def add_output(self, layers, small_bias=False, noisy=False):\n",
    "        \n",
    "        '''\n",
    "        Add 2 ouput neurons\n",
    "        '''\n",
    "        \n",
    "        # we're going to deal with the last layer\n",
    "        index = -1\n",
    "        \n",
    "        # take a copy of the current weights stored in self._fc\n",
    "        old_weight = layers[index].weight.data\n",
    "        old_bias   = layers[index].bias.data\n",
    "        device     = old_weight.device\n",
    "        # randomly initialize a tensor with the size of the wanted layer\n",
    "        \n",
    "        \n",
    "        new_weight = torch.zeros([2, old_weight.shape[1]]).to(device)\n",
    "        bound = 1 / math.sqrt(old_weight.shape[1])\n",
    "        nn.init.uniform_(new_weight, -bound, bound)\n",
    "        \n",
    "        # since we're adding new output neurons, new biases are needed\n",
    "        new_bias   = torch.zeros(2).to(device)\n",
    "        if small_bias:\n",
    "#             constant = old_bias.min()*10 if old_bias.min() < 0 else 0\n",
    "            nn.init.constant_(new_bias, -100)\n",
    "        else:\n",
    "            nn.init.uniform_(new_bias, -bound, bound)\n",
    "        \n",
    "        # concatenate the old weights with the new weights\n",
    "        new_wi = torch.cat([old_weight, new_weight], dim=0)\n",
    "        new_bi = torch.cat([old_bias, new_bias])\n",
    "\n",
    "        # reset weight and grad variables to new size\n",
    "        layers[index] = nn.Linear(old_weight.shape[1], old_weight.shape[0]+2).to(device) if not noisy \\\n",
    "                        else NoisyLinear(old_weight.shape[1], old_weight.shape[0]+2).to(device)\n",
    "\n",
    "        # set the weight data to new values\n",
    "        layers[index].weight = torch.nn.Parameter(new_wi).to(device)\n",
    "        layers[index].bias   = torch.nn.Parameter(new_bi).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is9ZRvdWyfnY"
   },
   "source": [
    "## environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvrstIXZyfnZ"
   },
   "outputs": [],
   "source": [
    "class environment():\n",
    "    def __init__(self, max_dim, cur_dim, window, length, maxSteps):\n",
    "        self.max_dim = max_dim\n",
    "        self.cur_dim = cur_dim\n",
    "        self.window = window\n",
    "        self.length = length # length of each dimension\n",
    "        self.maxSteps = maxSteps # max length of an episode\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "        \n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.reward = 0\n",
    "        # second dimension for pos/rewards concatination, third dimension for window concatination\n",
    "        self.pos = np.zeros((self.cur_dim, 1, 1)) \n",
    "        self.rewards = np.ones((self.cur_dim, 1, 1))\n",
    "        # previous states\n",
    "        self.history = deque([np.concatenate((np.zeros((cur_dim, 1, 1)), np.ones((cur_dim, 1, 1))), axis=1) for _ in range(window)], maxlen = window)\n",
    "        \n",
    "        # return a sequence of states\n",
    "        return np.concatenate(self.history, axis=2).flatten()\n",
    "        \n",
    "    def step(self, action):\n",
    "        delta = 1/(self.length-1)\n",
    "        self.steps += 1\n",
    "        r = 0 # reward\n",
    "        \n",
    "        if action == 0:    #NOOP\n",
    "            r = 0\n",
    "        else:\n",
    "            # deduction for the first action (NOOP)\n",
    "            action = action - 1\n",
    "            \n",
    "            # dimension \n",
    "            d = action // 2\n",
    "            \n",
    "            if action % 2 == 0:\n",
    "                self.pos[d] = max(0, self.pos[d] - delta)\n",
    "            else:\n",
    "                self.pos[d] = min(length, self.pos[d] + delta)\n",
    "        \n",
    "        # calculate reward\n",
    "        # gives reward when the agent is on the very end of any dimension\n",
    "        # reward is given only when the reward on the dimension is still unclaimed\n",
    "        for i in range(self.cur_dim):\n",
    "            if (self.pos[i] == length) and (sum(self.pos) == length and (self.rewards[i] == 1) ):\n",
    "                self.rewards[i] = 0\n",
    "                r = 1\n",
    "                \n",
    "        \n",
    "        self.reward += r\n",
    "        \n",
    "        #update history\n",
    "        self.history.append(np.concatenate((self.pos, self.rewards), axis=1))\n",
    "        \n",
    "        # check if step limit reached or all rewards claimed\n",
    "        # if so, return 1 for environment end\n",
    "        return np.concatenate(self.history, axis=2).flatten(), r, (self.steps >= self.maxSteps or self.reward == self.cur_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouv23glgf5Qt"
   },
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qtb3TRKcyfnZ"
   },
   "outputs": [],
   "source": [
    "def test(policy, max_dim, cur_dim, length, num_test, epsilon_test, window, max_steps):\n",
    "    total_reward = 0\n",
    "    policy.eval()\n",
    "    \n",
    "    for i in range(num_test):\n",
    "        # environment setup\n",
    "        env = environment(max_dim, cur_dim, window, length, max_steps)\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state)\n",
    "        \n",
    "        # run for one episode\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                q = policy(state.to(device))\n",
    "            if np.random.random() < epsilon_test:\n",
    "                action = torch.randint(low=0, high=(cur_dim*2)+1, size=(1,))\n",
    "            else:\n",
    "                action = q[0:(cur_dim*2)+1].argmax(dim=-1)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            state = next_state\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        total_reward += env.reward\n",
    "    \n",
    "    policy.train()\n",
    "    \n",
    "    return total_reward / num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CImOt0cyfna"
   },
   "outputs": [],
   "source": [
    "# statistics\n",
    "# ld for linear decay\n",
    "stats_gamma07 = {'e-greedy':[], 'retrain':[], 'DDE':[], \"softmax\":[], \"argmin\":[], \"RND\":[], \"noisy\":[], \"UCB\":[], \"DAE\":[], \"DDE2\":[], \"e-greedy-ld\":[], \"DAE-ld\":[]}\n",
    "\n",
    "# load previous stats is available\n",
    "# with open('stats_gamma07.pkl', 'rb') as f:\n",
    "#     stats_gamma07 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y48vOTJ9yfna",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_trials = 20\n",
    "max_training_steps = 200000 # total training steps\n",
    "max_steps = 500 # steps per episode\n",
    "max_dim = 10\n",
    "cur_dim = 1\n",
    "length = 2 # length of each dimension\n",
    "num_test = 1 \n",
    "test_freq = max_training_steps / 100\n",
    "gamma = 0.7\n",
    "epsilon = 0.1\n",
    "epsilon_test = 0.01\n",
    "delta = (1-epsilon)/5000. # for linear decay epsilon-greedy\n",
    "window = 4\n",
    "lr = 3e-4\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Q-learning\n",
    "min_memory = 250\n",
    "max_memory = 10000\n",
    "batch_size = 32\n",
    "target_freq = 300\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# DAE setting\n",
    "lr_DAE = 1e-3\n",
    "td_threshold = 0.5\n",
    "max_epsilon = 0.2\n",
    "linear_decay = (1-epsilon) / 5000.\n",
    "\n",
    "ucb_c   = 1\n",
    "\n",
    "# DDE for Dynamic Directed epsilon-greedy Exploration: DAE without the meta-policy\n",
    "# ld for linear decay\n",
    "for method in [\"DAE\", 'e-greedy', 'retrain', 'DDE', \"argmin\", \"softmax\", \"noisy\", \"UCB\", \"RND\", \"DAE-ld\", \"e-greedy-ld\"]:\n",
    "    for t in range(num_trials):\n",
    "        if method == \"e-greedy-ld\" or method == \"DAE-ld\":\n",
    "            epsilon = 1\n",
    "            \n",
    "        print(f\"method: {method}, trial: {t+1}/{num_trials}\")\n",
    "        \n",
    "        agent = DQN(dim = cur_dim, window = window, noisy = method==\"noisy\").to(device)\n",
    "        agent_target = copy.deepcopy(agent).to(device)\n",
    "        optimizer_policy  = optim.Adam(list(agent.policy.parameters())+list(agent.predictor.parameters()), lr=lr)\n",
    "        optimizer_DAE = optim.Adam(list(agent.explorer.parameters())+list(agent.meta_policy.parameters()), lr=lr_DAE)\n",
    "        \n",
    "        env = environment(max_dim, cur_dim, window, length, max_steps)\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state)\n",
    "\n",
    "        #init memory\n",
    "        s_memory   = torch.zeros(max_memory, cur_dim * window * 2)\n",
    "        a_memory   = torch.zeros(max_memory)\n",
    "        r_memory   = torch.zeros(max_memory)\n",
    "        ns_memory  = torch.zeros(max_memory, cur_dim * window * 2)\n",
    "        num_memory = 0\n",
    "        memory_idx = 0\n",
    "        \n",
    "        ucb_cnt = torch.zeros(cur_dim*2 + 1).to(device) + 1e-5\n",
    "\n",
    "        # record the number of steps each dimension takes to be well-learned (receiving max rewards in testing)\n",
    "        learning_curve = [max_training_steps for _ in range(max_dim)]\n",
    "        # records the relative frequency and epsilon of DAE\n",
    "        rf_curve = []\n",
    "        ep_curve_new = [0]\n",
    "        ep_curve_old = [0]\n",
    "        \n",
    "        for step in tqdm(range(max_training_steps)):\n",
    "            #evaluate the agent\n",
    "            if (step+1) % test_freq == 0:\n",
    "                test_reward = test(agent, max_dim, cur_dim, length, num_test, epsilon_test, window, max_steps)\n",
    "                if test_reward >= cur_dim: # passed the dimension\n",
    "                    learning_curve[cur_dim-1] = step+1\n",
    "                    print(f\"level {cur_dim} passed !!!!\")\n",
    "                    if cur_dim == max_dim:\n",
    "                        break\n",
    "                    else:\n",
    "                        if method == \"e-greedy-ld\" or method == \"DAE-ld\":\n",
    "                            epsilon = 1\n",
    "                        cur_dim = cur_dim + 1\n",
    "                        agent.add_dim()\n",
    "                        agent = agent.to(device)\n",
    "                        agent_target.add_dim()\n",
    "                        agent_target.to(device)\n",
    "                        \n",
    "                        if method == \"noisy\":\n",
    "                            for module in agent.policy:\n",
    "                                if isinstance(module, NoisyLinear):\n",
    "                                    module.reset_noise_stream()\n",
    "                                    \n",
    "                        ucb_cnt = torch.cat((ucb_cnt, torch.zeros(2).to(device)+1e-5))\n",
    "                        optimizer_policy  = optim.Adam(list(agent.policy.parameters())+list(agent.predictor.parameters()), lr=lr)\n",
    "                        optimizer_DAE = optim.Adam(list(agent.explorer.parameters())+list(agent.meta_policy.parameters()), lr=lr_DAE)\n",
    "                        env = environment(max_dim, cur_dim, window, length, max_steps)\n",
    "                        \n",
    "                        # append zeros to the states in the replay buffer\n",
    "                        s_memory  = torch.cat([s_memory, torch.zeros(max_memory, window*2)], dim=1)\n",
    "                        ns_memory = torch.cat([ns_memory, torch.zeros(max_memory, window*2)], dim=1)\n",
    "                        state = env.reset()\n",
    "                        state = torch.FloatTensor(state)\n",
    "                        if method == \"retrain\":\n",
    "                            agent = DQN(dim = cur_dim, window = window, noisy = method==\"noisy\").to(device)\n",
    "                            agent_target = copy.deepcopy(agent).to(device)\n",
    "                            optimizer_policy  = optim.Adam(list(agent.policy.parameters())+list(agent.predictor.parameters()), lr=lr)\n",
    "                            optimizer_DAE    = optim.Adam(list(agent.explorer.parameters())+list(agent.meta_policy.parameters()), lr=lr_DAE)\n",
    "\n",
    "            #interact\n",
    "            with torch.no_grad():\n",
    "                q = agent(state.to(device))\n",
    "\n",
    "                logit_freq = agent.novelty(state.to(device))\n",
    "                prob = nn.Softmax(dim=-1)(logit_freq)             \n",
    "                rf_curve.append(np.concatenate((prob.cpu().numpy(), np.zeros((max_dim-cur_dim)*2)), 0))\n",
    "                ep = nn.Softmax(dim=-1)(agent.meta_policy(state.to(device)))[1]\n",
    "                \n",
    "                # record current epsilon (according to the current state)\n",
    "                if (step+1)<=20000 or (np.concatenate(env.history, axis=2).flatten()[-8:-4] == 0).all():\n",
    "                    ep_curve_new.append(ep_curve_new[-1])\n",
    "                    ep_curve_old.append(ep.cpu().item())\n",
    "                elif(np.concatenate(env.history, axis=2).flatten()[-8:-4] != 0).all():\n",
    "                    ep_curve_new.append(ep.cpu().item())\n",
    "                    ep_curve_old.append(ep_curve_old[-1])\n",
    "                else:\n",
    "                    ep_curve_new.append(ep_curve_new[-1])\n",
    "                    ep_curve_old.append(ep_curve_old[-1])\n",
    "                \n",
    "            if method == \"softmax\":\n",
    "                action = Categorical(nn.Softmax(-1)(q[0:(cur_dim*2)+1])).sample()\n",
    "            elif method == \"UCB\":\n",
    "                action = (q+ucb_c*torch.sqrt(np.log(step+1)/ucb_cnt))[0:(cur_dim*2)+1].argmax(dim=-1)\n",
    "            elif method == \"DAE\":\n",
    "                if np.random.random() < min(max(epsilon, ep), max_epsilon):\n",
    "                    action = prob[0:(cur_dim*2)+1].argmin(dim=-1).long()\n",
    "                    exploration = True\n",
    "                else:\n",
    "                    action = q[0:(cur_dim*2)+1].argmax(dim=-1).long()\n",
    "                    exploration = False\n",
    "            elif method == \"DDE2\": # DAE without the explorer\n",
    "                if np.random.random() < min(max(epsilon, ep), max_epsilon):\n",
    "                    action = torch.randint(low=0, high=(cur_dim*2)+1, size=(1,)).long()\n",
    "                    exploration = True\n",
    "                else:\n",
    "                    action = q[0:(cur_dim*2)+1].argmax(dim=-1).long()\n",
    "                    exploration = False\n",
    "            elif np.random.random() < epsilon:\n",
    "                if method == \"e-greedy\" or method == \"retrain\" or method == \"e-greedy-ld\":\n",
    "                    action = torch.randint(low=0, high=(cur_dim*2)+1, size=(1,)).long()\n",
    "                elif method == \"DDE\" or method == \"DAE-ld\":\n",
    "                    action = prob[0:(cur_dim*2)+1].argmin(dim=-1).long()\n",
    "                elif method == \"argmin\":\n",
    "                    action = q[0:(cur_dim*2)+1].argmin(dim=-1).long()\n",
    "                else:\n",
    "                    action = q[0:(cur_dim*2)+1].argmax(dim=-1).long()\n",
    "            else:\n",
    "                action = q[0:(cur_dim*2)+1].argmax(dim=-1).long()\n",
    "            \n",
    "            #linear decay\n",
    "            epsilon = max(0.1, epsilon-delta)\n",
    "            \n",
    "            ucb_cnt[action] += 1\n",
    "                \n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            \n",
    "            if method == \"RND\":\n",
    "                prediction_error = agent.intrinsic(next_state.to(device))\n",
    "                reward = reward + prediction_error.item()\n",
    "                \n",
    "            #store memory\n",
    "            s_memory[memory_idx] = state\n",
    "            a_memory[memory_idx] = action\n",
    "            r_memory[memory_idx] = reward\n",
    "            ns_memory[memory_idx] = next_state\n",
    "            num_memory = min(max_memory, num_memory+1)\n",
    "            memory_idx = (memory_idx+1)%max_memory\n",
    "\n",
    "            #learn\n",
    "            if num_memory >= min_memory:\n",
    "                idx = random.choices(range(num_memory), k=batch_size)\n",
    "                s   = s_memory[idx].to(device)\n",
    "                a   = a_memory[idx].type(torch.long).to(device)\n",
    "                r   = r_memory[idx].to(device)\n",
    "                ns  = ns_memory[idx].to(device) \n",
    "                \n",
    "                \n",
    "                #DAE\n",
    "                logit_freq = agent.novelty(state.to(device))\n",
    "                logprob    = nn.LogSoftmax(dim=-1)(logit_freq)\n",
    "                prob       = nn.Softmax(dim=-1)(logit_freq)\n",
    "                \n",
    "                logit_epsilon = agent.tradeoff(state.to(device))\n",
    "                log_epsilon   = nn.LogSoftmax(dim=-1)(logit_epsilon)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    target_value = reward + gamma*agent_target(next_state.to(device))[0:(cur_dim*2)+1].max(dim=-1)[0]\n",
    "                    predict_value = agent(state.to(device))[action]\n",
    "                    temp_diff  = nn.L1Loss(reduction='none')(target_value, predict_value)\n",
    "                    temp_ratio = temp_diff / target_value\n",
    "                    temp_ratio = temp_ratio.cpu().detach().numpy()\n",
    "                \n",
    "                # DQN\n",
    "                target_values = r + gamma*agent_target(ns)[:,0:(cur_dim*2)+1].max(dim=-1)[0]\n",
    "                predict_values = agent(s)[range(batch_size), a]\n",
    "                \n",
    "                loss = loss_fn(target_values, predict_values)\n",
    "                \n",
    "                if method == \"DDE\" or method == \"DAE-ld\":\n",
    "                    loss -= logprob[action]\n",
    "                if method == \"RND\":\n",
    "                    loss += prediction_error\n",
    "                if method == \"DAE\":\n",
    "                    loss -= logprob[action]\n",
    "                    if exploration:\n",
    "                        if temp_ratio > td_threshold:\n",
    "                            loss -= log_epsilon[1]\n",
    "                        else:\n",
    "                            loss -= log_epsilon[0]\n",
    "\n",
    "\n",
    "                optimizer_policy.zero_grad()\n",
    "                optimizer_DAE.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_policy.step()\n",
    "                optimizer_DAE.step()\n",
    "\n",
    "            if (step+1)  % target_freq == 0:\n",
    "                agent_target = copy.deepcopy(agent)\n",
    "\n",
    "            state = next_state\n",
    "            #check if env done\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                state = torch.FloatTensor(state)\n",
    "        \n",
    "        stats_gamma07[method].append(learning_curve)\n",
    "        cur_dim = 1\n",
    "\n",
    "# save stats\n",
    "with open('stats_gamma07.pkl', 'wb') as f:\n",
    "    pickle.dump(stats_gamma07, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akXz3Vpiyfnb"
   },
   "outputs": [],
   "source": [
    "def adjust_lightness(color, amount=1):\n",
    "    import matplotlib.colors as mc\n",
    "    import colorsys\n",
    "    try:\n",
    "        c = mc.cnames[color]\n",
    "    except:\n",
    "        c = color\n",
    "    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n",
    "    return colorsys.hls_to_rgb(c[0], max(0, min(1, amount * c[1])), c[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFtrygm3yfnb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = [i for i in range(1, max_dim+1)]\n",
    "plt.figure(figsize=(12, 4.5))\n",
    "\n",
    "scale = 1000\n",
    "\n",
    "plt.plot(x, np.array(stats_gamma07[\"RND\"]).mean(0)/scale, label=\"RND\", color=adjust_lightness(\"tab:red\", 1.4), linewidth=5)\n",
    "plt.plot(x, np.array(stats_gamma07[\"noisy\"]).mean(0)/scale, label=\"NoisyNet\", color=adjust_lightness(\"tab:orange\", 1.4), linewidth=5)\n",
    "plt.plot(x, np.array(stats_gamma07[\"softmax\"]).mean(0)/scale, label=\"Softmax\", color=adjust_lightness(\"gold\", 1), linewidth=5)\n",
    "plt.plot(x, np.array(stats_gamma07[\"UCB\"]).mean(0)/scale, label=\"UCB\", color = adjust_lightness(\"tab:olive\", 1), linewidth=5)\n",
    "plt.plot(x, np.array(stats_gamma07[\"e-greedy\"]).mean(0)/scale, label=\"ϵ-greedy\", color = adjust_lightness(\"tab:green\", 1.6), linewidth=5)\n",
    "plt.plot(x, np.array(stats_gamma07[\"argmin\"]).mean(0)/scale, label=\"ArgminQ\", color = adjust_lightness(\"tab:blue\", 1.6), linewidth=5)\n",
    "# plt.plot(x, np.array(stats_gamma07[\"DDE\"]).mean(0), label=\"DDE(ours)\", color=\"blueviolet\", linewidth=5)\n",
    "plt.plot(x, np.array(stats_gamma07[\"DAE\"]).mean(0)/scale, label='DAE(ours)', color='blueviolet', linewidth=5)\n",
    "plt.scatter(x, np.array(stats_gamma07[\"retrain\"]).mean(0)/scale, marker='h', label=\"ϵ-greedy\\n(Retrain)\", color = \"gray\", zorder=10, s=128)\n",
    "plt.xlabel(\"#dimensions\", fontsize=24)\n",
    "plt.ylabel(\"training steps (k)\", fontsize=24)\n",
    "plt.xticks(fontsize=0)\n",
    "plt.yticks(fontsize=24)\n",
    "axis = plt.gca()\n",
    "axis.set_ylim([0, 150])\n",
    "plt.legend(loc='upper left', prop={'size': 17})\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"example.eps\", format=\"eps\")\n",
    "plt.savefig(\"example.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Analysis of DAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIGsQKaEyfna"
   },
   "outputs": [],
   "source": [
    "epsilon_curve = {\"new_states\":[], \"old_states\":[]}\n",
    "relative_frequency_curve = []\n",
    "        \n",
    "with open('epsilon_curve.pkl', 'rb') as f:\n",
    "    epsilon_curve = pickle.load(f)\n",
    "with open('relative_frequency_curve.pkl', 'rb') as f:\n",
    "    relative_frequency_curve = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y48vOTJ9yfna",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_trials = 20\n",
    "max_training_steps = 200000 # total training steps\n",
    "max_steps = 500 # steps per episode\n",
    "max_dim = 10\n",
    "cur_dim = 1\n",
    "length = 2 # length of each dimension\n",
    "num_test = 1 \n",
    "test_freq = max_training_steps / 100\n",
    "gamma = 0.7\n",
    "epsilon = 0.1\n",
    "epsilon_test = 0.01\n",
    "delta = (1-epsilon)/5000. # for linear decay epsilon-greedy\n",
    "window = 4\n",
    "lr = 3e-4\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Q-learning\n",
    "min_memory = 250\n",
    "max_memory = 10000\n",
    "batch_size = 32\n",
    "target_freq = 300\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# DAE setting\n",
    "lr_DAE = 1e-3\n",
    "td_threshold = 0.5\n",
    "max_epsilon = 0.2\n",
    "linear_decay = (1-epsilon) / 5000.\n",
    "\n",
    "ucb_c   = 1\n",
    "\n",
    "# DDE for Dynamic Directed epsilon-greedy Exploration: DAE without the meta-policy\n",
    "# ld for linear decay\n",
    "for method in [\"DAE\"]:\n",
    "    for t in range(num_trials):\n",
    "        if method == \"e-greedy-ld\" or method == \"DAE-ld\":\n",
    "            epsilon = 1\n",
    "            \n",
    "        print(f\"method: {method}, trial: {t+1}/{num_trials}\")\n",
    "        \n",
    "        agent = DQN(dim = cur_dim, window = window, noisy = method==\"noisy\").to(device)\n",
    "        agent_target = copy.deepcopy(agent).to(device)\n",
    "        optimizer_policy  = optim.Adam(list(agent.policy.parameters())+list(agent.predictor.parameters()), lr=lr)\n",
    "        optimizer_DAE = optim.Adam(list(agent.explorer.parameters())+list(agent.meta_policy.parameters()), lr=lr_DAE)\n",
    "        \n",
    "        env = environment(max_dim, cur_dim, window, length, max_steps)\n",
    "        state = env.reset()\n",
    "        state = torch.FloatTensor(state)\n",
    "\n",
    "        #init memory\n",
    "        s_memory   = torch.zeros(max_memory, cur_dim * window * 2)\n",
    "        a_memory   = torch.zeros(max_memory)\n",
    "        r_memory   = torch.zeros(max_memory)\n",
    "        ns_memory  = torch.zeros(max_memory, cur_dim * window * 2)\n",
    "        num_memory = 0\n",
    "        memory_idx = 0\n",
    "        \n",
    "        ucb_cnt = torch.zeros(cur_dim*2 + 1).to(device) + 1e-5\n",
    "\n",
    "        # record the number of steps each dimension takes to be well-learned (receiving max rewards in testing)\n",
    "        learning_curve = [max_training_steps for _ in range(max_dim)]\n",
    "        # records the relative frequency and epsilon of DAE\n",
    "        rf_curve = []\n",
    "        ep_curve_new = [0]\n",
    "        ep_curve_old = [0]\n",
    "        \n",
    "        for step in tqdm(range(max_training_steps)):\n",
    "            #evaluate the agent\n",
    "            if (step+1) % test_freq == 0:\n",
    "                test_reward = test(agent, max_dim, cur_dim, length, num_test, epsilon_test, window, max_steps)\n",
    "#                 if test_reward >= cur_dim: # passed the dimension\n",
    "                if (step+1) % 20000 == 0:\n",
    "                    learning_curve[cur_dim-1] = step+1\n",
    "                    if cur_dim == max_dim:\n",
    "                        break\n",
    "                    else:\n",
    "                        if method == \"e-greedy-ld\" or method == \"DAE-ld\":\n",
    "                            epsilon = 1\n",
    "                        cur_dim = cur_dim + 1\n",
    "                        agent.add_dim()\n",
    "                        agent = agent.to(device)\n",
    "                        agent_target.add_dim()\n",
    "                        agent_target.to(device)\n",
    "                        \n",
    "                        if method == \"noisy\":\n",
    "                            for module in agent.policy:\n",
    "                                if isinstance(module, NoisyLinear):\n",
    "                                    module.reset_noise_stream()\n",
    "                                    \n",
    "                        ucb_cnt = torch.cat((ucb_cnt, torch.zeros(2).to(device)+1e-5))\n",
    "                        optimizer_policy  = optim.Adam(list(agent.policy.parameters())+list(agent.predictor.parameters()), lr=lr)\n",
    "                        optimizer_DAE = optim.Adam(list(agent.explorer.parameters())+list(agent.meta_policy.parameters()), lr=lr_DAE)\n",
    "                        env = environment(max_dim, cur_dim, window, length, max_steps)\n",
    "                        \n",
    "                        # append zeros to the states in the replay buffer\n",
    "                        s_memory  = torch.cat([s_memory, torch.zeros(max_memory, window*2)], dim=1)\n",
    "                        ns_memory = torch.cat([ns_memory, torch.zeros(max_memory, window*2)], dim=1)\n",
    "                        state = env.reset()\n",
    "                        state = torch.FloatTensor(state)\n",
    "                        if method == \"retrain\":\n",
    "                            agent = DQN(dim = cur_dim, window = window, noisy = method==\"noisy\").to(device)\n",
    "                            agent_target = copy.deepcopy(agent).to(device)\n",
    "                            optimizer_policy  = optim.Adam(list(agent.policy.parameters())+list(agent.predictor.parameters()), lr=lr)\n",
    "                            optimizer_DAE    = optim.Adam(list(agent.explorer.parameters())+list(agent.meta_policy.parameters()), lr=lr_DAE)\n",
    "\n",
    "            #interact\n",
    "            with torch.no_grad():\n",
    "                q = agent(state.to(device))\n",
    "\n",
    "                logit_freq = agent.novelty(state.to(device))\n",
    "                prob = nn.Softmax(dim=-1)(logit_freq)             \n",
    "                rf_curve.append(np.concatenate((prob.cpu().numpy(), np.zeros((max_dim-cur_dim)*2)), 0))\n",
    "                ep = nn.Softmax(dim=-1)(agent.meta_policy(state.to(device)))[1]\n",
    "                \n",
    "                # record current epsilon (according to the current state)\n",
    "                if (step+1)<=20000 or (np.concatenate(env.history, axis=2).flatten()[-8:-4] == 0).all():\n",
    "                    ep_curve_new.append(ep_curve_new[-1])\n",
    "                    ep_curve_old.append(ep.cpu().item())\n",
    "                elif(np.concatenate(env.history, axis=2).flatten()[-8:-4] != 0).all():\n",
    "                    ep_curve_new.append(ep.cpu().item())\n",
    "                    ep_curve_old.append(ep_curve_old[-1])\n",
    "                else:\n",
    "                    ep_curve_new.append(ep_curve_new[-1])\n",
    "                    ep_curve_old.append(ep_curve_old[-1])\n",
    "                \n",
    "            if method == \"softmax\":\n",
    "                action = Categorical(nn.Softmax(-1)(q[0:(cur_dim*2)+1])).sample()\n",
    "            elif method == \"UCB\":\n",
    "                action = (q+ucb_c*torch.sqrt(np.log(step+1)/ucb_cnt))[0:(cur_dim*2)+1].argmax(dim=-1)\n",
    "            elif method == \"DAE\":\n",
    "                if np.random.random() < min(max(epsilon, ep), max_epsilon):\n",
    "                    action = prob[0:(cur_dim*2)+1].argmin(dim=-1).long()\n",
    "                    exploration = True\n",
    "                else:\n",
    "                    action = q[0:(cur_dim*2)+1].argmax(dim=-1).long()\n",
    "                    exploration = False\n",
    "            elif method == \"DDE2\": # DAE without the explorer\n",
    "                if np.random.random() < min(max(epsilon, ep), max_epsilon):\n",
    "                    action = torch.randint(low=0, high=(cur_dim*2)+1, size=(1,)).long()\n",
    "                    exploration = True\n",
    "                else:\n",
    "                    action = q[0:(cur_dim*2)+1].argmax(dim=-1).long()\n",
    "                    exploration = False\n",
    "            elif np.random.random() < epsilon:\n",
    "                if method == \"e-greedy\" or method == \"retrain\" or method == \"e-greedy-ld\":\n",
    "                    action = torch.randint(low=0, high=(cur_dim*2)+1, size=(1,)).long()\n",
    "                elif method == \"DDE\" or method == \"DAE-ld\":\n",
    "                    action = prob[0:(cur_dim*2)+1].argmin(dim=-1).long()\n",
    "                elif method == \"argmin\":\n",
    "                    action = q[0:(cur_dim*2)+1].argmin(dim=-1).long()\n",
    "                else:\n",
    "                    action = q[0:(cur_dim*2)+1].argmax(dim=-1).long()\n",
    "            else:\n",
    "                action = q[0:(cur_dim*2)+1].argmax(dim=-1).long()\n",
    "            \n",
    "            #linear decay\n",
    "            epsilon = max(0.1, epsilon-delta)\n",
    "            \n",
    "            ucb_cnt[action] += 1\n",
    "                \n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            \n",
    "            if method == \"RND\":\n",
    "                prediction_error = agent.intrinsic(next_state.to(device))\n",
    "                reward = reward + prediction_error.item()\n",
    "                \n",
    "            #store memory\n",
    "            s_memory[memory_idx] = state\n",
    "            a_memory[memory_idx] = action\n",
    "            r_memory[memory_idx] = reward\n",
    "            ns_memory[memory_idx] = next_state\n",
    "            num_memory = min(max_memory, num_memory+1)\n",
    "            memory_idx = (memory_idx+1)%max_memory\n",
    "\n",
    "            #learn\n",
    "            if num_memory >= min_memory:\n",
    "                idx = random.choices(range(num_memory), k=batch_size)\n",
    "                s   = s_memory[idx].to(device)\n",
    "                a   = a_memory[idx].type(torch.long).to(device)\n",
    "                r   = r_memory[idx].to(device)\n",
    "                ns  = ns_memory[idx].to(device) \n",
    "                \n",
    "                \n",
    "                #DAE\n",
    "                logit_freq = agent.novelty(state.to(device))\n",
    "                logprob    = nn.LogSoftmax(dim=-1)(logit_freq)\n",
    "                prob       = nn.Softmax(dim=-1)(logit_freq)\n",
    "                \n",
    "                logit_epsilon = agent.tradeoff(state.to(device))\n",
    "                log_epsilon   = nn.LogSoftmax(dim=-1)(logit_epsilon)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    target_value = reward + gamma*agent_target(next_state.to(device))[0:(cur_dim*2)+1].max(dim=-1)[0]\n",
    "                    predict_value = agent(state.to(device))[action]\n",
    "                    temp_diff  = nn.L1Loss(reduction='none')(target_value, predict_value)\n",
    "                    temp_ratio = temp_diff / target_value\n",
    "                    temp_ratio = temp_ratio.cpu().detach().numpy()\n",
    "                \n",
    "                # DQN\n",
    "                target_values = r + gamma*agent_target(ns)[:,0:(cur_dim*2)+1].max(dim=-1)[0]\n",
    "                predict_values = agent(s)[range(batch_size), a]\n",
    "                \n",
    "                loss = loss_fn(target_values, predict_values)\n",
    "                \n",
    "                if method == \"DDE\" or method == \"DAE-ld\":\n",
    "                    loss -= logprob[action]\n",
    "                if method == \"RND\":\n",
    "                    loss += prediction_error\n",
    "                if method == \"DAE\":\n",
    "                    loss -= logprob[action]\n",
    "                    if exploration:\n",
    "                        if temp_ratio > td_threshold:\n",
    "                            loss -= log_epsilon[1]\n",
    "                        else:\n",
    "                            loss -= log_epsilon[0]\n",
    "\n",
    "\n",
    "                optimizer_policy.zero_grad()\n",
    "                optimizer_DAE.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_policy.step()\n",
    "                optimizer_DAE.step()\n",
    "\n",
    "            if (step+1)  % target_freq == 0:\n",
    "                agent_target = copy.deepcopy(agent)\n",
    "\n",
    "            state = next_state\n",
    "            #check if env done\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                state = torch.FloatTensor(state)\n",
    "        \n",
    "        relative_frequency_curve.append(rf_curve)\n",
    "        epsilon_curve[\"new_states\"].append(ep_curve_new[1:])\n",
    "        epsilon_curve[\"old_states\"].append(ep_curve_old[1:])\n",
    "        cur_dim = 1\n",
    "\n",
    "# save stats\n",
    "with open('epsilon_curve.pkl', 'wb') as f:\n",
    "    pickle.dump(epsilon_curve, f)\n",
    "with open('relative_frequency_curve.pkl', 'wb') as f:\n",
    "    pickle.dump(relative_frequency_curve, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxlq8hbZyfnb"
   },
   "source": [
    "# Relative Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 1080,
     "status": "error",
     "timestamp": 1678372434210,
     "user": {
      "displayName": "丁維",
      "userId": "15192463341450164586"
     },
     "user_tz": -480
    },
    "id": "2LekiaRmyfnc",
    "outputId": "16fc19e5-bfaf-49fb-de86-3413cc34edf0"
   },
   "outputs": [],
   "source": [
    "n = len(relative_frequency_curve)\n",
    "max_len = max([len(c) for c in relative_frequency_curve])\n",
    "arr = np.ma.empty((n, max_len, 21))\n",
    "arr.mask = True\n",
    "for i in range(len(relative_frequency_curve[:10])):\n",
    "    data = np.array(relative_frequency_curve[i])\n",
    "    arr[i, :data.shape[0], :data.shape[1]] = data\n",
    "rf_curve_mean = arr.mean(axis = 0).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HljVfNZqyfnc"
   },
   "outputs": [],
   "source": [
    "# divide actions into action groups (A1~A10)\n",
    "rf_curve_mean = [rf_curve_mean[:, :3].mean(1) if i == 0 else rf_curve_mean[:, 3+(i-1)*2:3+(i)*2].mean(1) for i in range(max_dim)]\n",
    "rf_curve_mean = np.array(rf_curve_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average = 10000\n",
    "moving_averages = []\n",
    "for i in range(len(rf_curve_mean)):\n",
    "    ma = np.array([rf_curve_mean[i][:j+1].mean() if j < moving_average else rf_curve_mean[i][j+1-moving_average:j+1].mean() for j in range(len(rf_curve_mean[i]))])\n",
    "    moving_averages.append(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXJeMLQCyfnc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = [i for i in range(len(rf_curve_mean[1]))]\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "pass_time = [i for i in range(20000, 200001, 20000)]\n",
    "max_prob  = 0\n",
    "\n",
    "for i, ma in enumerate(moving_averages):\n",
    "    max_prob = max(ma.max(), max_prob)\n",
    "    plt.plot(x, ma, label=f'A{i+1}', linewidth=5)\n",
    "    \n",
    "for i in range(len(pass_time)-1):\n",
    "    plt.plot([pass_time[i], pass_time[i]], [0, max_prob], linestyle = 'dashed', color='black')\n",
    "    \n",
    "\n",
    "plt.xlabel(\"training steps\", fontsize=24)\n",
    "plt.ylabel(\"$\\it{Relative Frequency}$\", fontsize=24)\n",
    "axis = plt.gca()\n",
    "axis.set_xlim([0, 200000])\n",
    "axis.axes.xaxis.set_ticklabels([])\n",
    "plt.yticks(fontsize= 24)\n",
    "\n",
    "plt.legend(loc='upper left', prop={'size': 17})\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"RelativeFrequency.eps\", format=\"eps\")\n",
    "plt.savefig(\"RelativeFrequency.png\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejp-dHnryfnc"
   },
   "source": [
    "# Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfok1wOXyfnc"
   },
   "outputs": [],
   "source": [
    "n = len(epsilon_curve[\"new_states\"])\n",
    "max_len = 200000\n",
    "arr = np.ma.empty((n, max_len))\n",
    "arr.mask = True\n",
    "for i, curve in enumerate(epsilon_curve[\"new_states\"]):\n",
    "    arr[i, :len(curve)] = curve\n",
    "new_mean = arr.mean(axis = 0).data\n",
    "\n",
    "\n",
    "n = len(epsilon_curve[\"old_states\"])\n",
    "\n",
    "max_len = 200000\n",
    "arr = np.ma.empty((n, max_len))\n",
    "arr.mask = True\n",
    "for i, curve in enumerate(epsilon_curve[\"old_states\"]):\n",
    "    arr[i, :len(curve)] = curve\n",
    "old_mean = arr.mean(axis = 0).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5KmCCRqyfnc"
   },
   "outputs": [],
   "source": [
    "x = [i for i in range(len(new_mean))]\n",
    "plt.figure(figsize=(12, 3))\n",
    "\n",
    "pass_time = [i for i in range(20000, 200001, 20000)]\n",
    "max_prob  = 0\n",
    "moving_average = 1000\n",
    "\n",
    "ma = np.array([new_mean[:j+1].mean() if j < moving_average else new_mean[j+1-moving_average:j+1].mean() for j in range(len(new_mean))])\n",
    "max_prob = max(ma.max(), max_prob)\n",
    "ma[:10000] = 0\n",
    "plt.plot(x, ma, label='new states', linewidth=5)\n",
    "\n",
    "ma = np.array([old_mean[:j+1].mean() if j < moving_average else old_mean[j+1-moving_average:j+1].mean() for j in range(len(old_mean))])\n",
    "max_prob = max(ma.max(), max_prob)\n",
    "plt.plot(x, ma, label='old states', linewidth=5)\n",
    "    \n",
    "for i in range(len(pass_time)-1):\n",
    "    plt.plot([pass_time[i], pass_time[i]], [0, max_prob], linestyle = 'dashed', color='black')\n",
    "    \n",
    "\n",
    "plt.xlabel(\"training steps\", fontsize=24)\n",
    "plt.ylabel(\"ϵ$_t$\", fontsize=28)\n",
    "axis = plt.gca()\n",
    "axis.set_xlim([0, 200000])\n",
    "axis.axes.xaxis.set_ticklabels([])\n",
    "axis.axes.get_xaxis().set_visible(False)\n",
    "plt.yticks(fontsize= 24)\n",
    "\n",
    "plt.legend(loc='upper left', prop={'size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"epsilon_curve.eps\", format=\"eps\")\n",
    "plt.savefig(\"epsilon_curve.png\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/ga642381/ML2021-Spring/blob/main/HW12/HW12_ZH.ipynb",
     "timestamp": 1624264979931
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
