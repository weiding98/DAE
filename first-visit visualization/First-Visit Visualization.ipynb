{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-Visit Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nl2nREINDLiw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import gym.spaces as spaces\n",
    "import random\n",
    "from scipy.interpolate import griddata\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explorer(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_space, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_space)\n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "    def forward(self, state):\n",
    "        hid = self.ReLU(self.fc1(state))\n",
    "        hid = self.ReLU(self.fc2(hid))\n",
    "        return F.softmax(self.fc3(hid), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, network, lr):\n",
    "        self.network = network\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "    \n",
    "    def learn(self, log_probs):\n",
    "        loss = log_probs.sum() #decease the probability of taken actions\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def sample(self, state):\n",
    "        action_prob = self.network(state)\n",
    "        action_dist = Categorical(action_prob)\n",
    "#         action = action_dist.sample()\n",
    "        action = action_prob.argmax()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def save(self, PATH):\n",
    "        Agent_Dict = {\n",
    "            \"network\" : self.network.state_dict(),\n",
    "            \"optimizer\" : self.optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(Agent_Dict, PATH)\n",
    "\n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.network.load_state_dict(checkpoint[\"network\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    \n",
    "    def __init__(self, size=23, max_steps=5000):\n",
    "        self.size = size\n",
    "        self.row  = size-1\n",
    "        self.col  = size//2\n",
    "        self.steps = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=size-1, shape=(2,), dtype=np.int)\n",
    "        \n",
    "    def step(self, action):\n",
    "        ''' 0: up, 1: down, 2:left, 3: right '''\n",
    "        if(action == 0):\n",
    "            self.row += 1\n",
    "        elif(action == 1):\n",
    "            self.row -= 1\n",
    "        elif(action == 2):\n",
    "            self.col -= 1\n",
    "        elif(action == 3):\n",
    "            self.col += 1\n",
    "            \n",
    "        self.row = min(max(self.row, 0), self.size-1)\n",
    "        self.col = min(max(self.col, 0), self.size-1)\n",
    "        self.steps += 1\n",
    "        info={}\n",
    "        \n",
    "        return (self.row, self.col), 0, self.steps >= self.max_steps, info\n",
    "    \n",
    "    def reset(self):\n",
    "        # put the agent to the upper-mid of the world\n",
    "        self.row = self.size-1\n",
    "        self.col = self.size//2\n",
    "        self.steps = 0\n",
    "        return (self.row, self.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "''' For first-visit stats'''\n",
    "first_visits = {'ϵ-greedy':[], 'ϵz-greedy':[], 'DAE(ours)':[]}\n",
    "\n",
    "hidden_size = 512\n",
    "lr = 1e-4\n",
    "NUM_EPISODE = 50\n",
    "grid_size = 23\n",
    "max_steps = 5000\n",
    "mu = 2\n",
    "\n",
    "env = GridWorld(size=grid_size, max_steps=max_steps)\n",
    "action_space = 4\n",
    "obs_space = 2\n",
    "for method in ['ϵ-greedy', 'ϵz-greedy', 'DAE(ours)']:\n",
    "    for e in range(NUM_EPISODE):\n",
    "        print(f'epoch {e+1}/{NUM_EPISODE}', end = '    \\r')\n",
    "\n",
    "        novelty_estimator_network = NoveltyEstimatorNetwork(obs_space, action_space, hidden_size).to(device)\n",
    "        noveltyEstimator = NoveltyEstimator(novelty_estimator_network, lr)\n",
    "\n",
    "        state = env.reset()\n",
    "        first_visit = np.full((grid_size, grid_size), max_steps)\n",
    "        total_step = 1\n",
    "        duration = 0\n",
    "        while True:\n",
    "\n",
    "            ''' record first visit '''\n",
    "            row, col = state\n",
    "            if first_visit[row][col] == max_steps:\n",
    "                first_visit[row][col] = total_step\n",
    "            ''' end recording '''\n",
    "\n",
    "\n",
    "            '''e-greedy'''\n",
    "            if method == 'ϵ-greedy':\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "\n",
    "            '''ez-greedy'''\n",
    "            if method == 'ϵz-greedy':\n",
    "                if duration == 0:\n",
    "                    action   = env.action_space.sample()\n",
    "                    duration = np.random.zipf(mu, 1)\n",
    "                duration -= 1\n",
    "\n",
    "\n",
    "            if method == 'DAE(ours)':\n",
    "                #normalize state\n",
    "                state = (row/grid_size, col/grid_size)\n",
    "                action, log_prob = noveltyEstimator.sample(torch.FloatTensor(state).to(device))\n",
    "                noveltyEstimator.learn(log_prob)\n",
    "\n",
    "\n",
    "            '''next state'''\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_step += 1\n",
    "            if done or not (first_visit==max_steps).any():\n",
    "                break\n",
    "        first_visits[method].append(first_visit)\n",
    "\n",
    "    first_visits[method] = np.array(first_visits[method])\n",
    "    first_visits[method] = first_visits[method].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contourfs = []\n",
    "fig = plt.figure(figsize=(9,3))\n",
    "for i, method in enumerate(['ϵ-greedy', 'ϵz-greedy', 'DAE(ours)']):\n",
    "    fig.add_subplot(1,3,i+1)\n",
    "    plt.contour (first_visits[method], vmin=0, vmax=max_steps ,cmap='jet', linewidths=2)\n",
    "    c = plt.contourf(first_visits[method], vmin=0, vmax=max_steps ,cmap='jet', alpha=0.6)\n",
    "    contourfs.append(c)\n",
    "\n",
    "    plt.title(method, fontsize=24)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./GridWorld/GridWorld.png', format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouv23glgf5Qt"
   },
   "source": [
    "## MountainCar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "\n",
    "        new_mean = self.mean + delta * batch_count / tot_count\n",
    "        m_a = self.var * (self.count)\n",
    "        m_b = batch_var * (batch_count)\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n",
    "        new_var = M2 / (self.count + batch_count)\n",
    "\n",
    "        new_count = batch_count + self.count\n",
    "\n",
    "        self.mean = new_mean\n",
    "        self.var = new_var\n",
    "        self.count = new_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' For first-visit stats'''\n",
    "first_visits = {'ϵ-greedy':[], 'ϵz-greedy':[], 'DAE(ours)':[]}\n",
    "\n",
    "V_MIN, V_MAX = -0.07, 0.07\n",
    "X_MIN, X_MAX = -1.2 , 0.6\n",
    "\n",
    "max_steps = 5000\n",
    "mu = 2\n",
    "NUM_BURN_IN = 10\n",
    "NUM_EPISODE = 50\n",
    "hidden_size = 512\n",
    "lr = 1e-4\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env._max_episode_steps = max_steps\n",
    "\n",
    "obs_space = 2\n",
    "action_space = 3\n",
    "\n",
    "# burn-in for observation normalization parameters\n",
    "obs_rms = RunningMeanStd(shape=(2,))\n",
    "for e in range(NUM_BURN_IN):\n",
    "    print(f'burn-in {e+1}/{NUM_BURN_IN}', end = '    \\r')\n",
    "    \n",
    "    explorer = Explorer(obs_space, action_space, hidden_size=hidden_size).cuda()\n",
    "    agent = Agent(explorer, lr=lr)\n",
    "\n",
    "    state = env.reset()\n",
    "    total_step = 1\n",
    "    while True:\n",
    "#         update obs normalization parameters\n",
    "        state = np.array(state)\n",
    "        state = np.expand_dims(state, 0)\n",
    "        obs_rms.update(state)\n",
    "#         normalize state\n",
    "        state -= obs_rms.mean\n",
    "        state /= np.sqrt(obs_rms.var)\n",
    "        action, log_prob = agent.sample(torch.FloatTensor(state).cuda())\n",
    "        agent.learn(log_prob)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_step += 1\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "print(obs_rms.mean, np.sqrt(obs_rms.var))\n",
    "\n",
    "for method in ['ϵ-greedy', 'ϵz-greedy', 'DAE(ours)']:\n",
    "    for e in range(NUM_EPISODE):\n",
    "        print(f'episode {e+1}/{NUM_EPISODE}', end = '    \\r')\n",
    "        explorer = Explorer(obs_space, action_space, hidden_size=hidden_size).cuda()\n",
    "        agent = Agent(explorer, lr=lr)\n",
    "\n",
    "        state = env.reset()\n",
    "        first_visit = np.full((12,12), max_steps)\n",
    "        total_step = 1\n",
    "        duration = 0\n",
    "        while True:\n",
    "            ''' record first visit '''\n",
    "            position, velocity = state\n",
    "            pos = int((position - X_MIN) / ((X_MAX-X_MIN)/11.))\n",
    "            vel = int((velocity - V_MIN) / ((V_MAX-V_MIN)/11.))\n",
    "            if first_visit[pos][vel] == max_steps:\n",
    "                first_visit[pos][vel] = total_step\n",
    "            ''' end recording '''\n",
    "\n",
    "\n",
    "            '''e-greedy'''\n",
    "            if method == 'ϵ-greedy':\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "\n",
    "            '''ez-greedy'''\n",
    "            if method == 'ϵz-greedy':\n",
    "                if duration == 0:\n",
    "                    action   = env.action_space.sample()\n",
    "                    duration = np.random.zipf(mu, 1)\n",
    "                duration -= 1\n",
    "\n",
    "\n",
    "            '''ours'''\n",
    "            if method == 'DAE(ours)':\n",
    "                state = np.array(state)\n",
    "                state -= obs_rms.mean\n",
    "                state /= np.sqrt(obs_rms.var)\n",
    "                action, log_prob = agent.sample(torch.FloatTensor(state).cuda())\n",
    "                agent.learn(log_prob)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_step += 1\n",
    "            if done:\n",
    "                break\n",
    "        first_visits[method].append(first_visit)\n",
    "\n",
    "    '''drawing picture'''\n",
    "    first_visits[method] = np.array(first_visits[method])\n",
    "    first_visits[method] = first_visits[method].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contourfs = []\n",
    "fig = plt.figure(figsize=(9,3))\n",
    "for i, method in enumerate(['ϵ-greedy', 'ϵz-greedy', 'DAE(ours)']):\n",
    "    fig.add_subplot(1,3,i+1)\n",
    "    pts = []\n",
    "    for x in range(12):\n",
    "        for v in range(12):\n",
    "            pts.append([1.8/11*x-1.2, 0.14/11*v-0.07])\n",
    "    xi, yi = np.mgrid[-1.2:0.7:0.1, -0.07:0.07:0.01]\n",
    "    # grid the data.\n",
    "    zi = griddata(pts, first_visits[method].flatten(), (xi, yi), method='linear')\n",
    "\n",
    "    plt.contour (xi, yi, zi, levels=8, vmin=0, vmax=max_steps ,cmap='jet', linewidths=2)\n",
    "    plt.contourf(xi, yi, zi, levels=8, vmin=0, vmax=max_steps, cmap='jet', alpha=0.6)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.plot([0.5, 0.5], [-0.07, 0.07], linestyle = 'dashed', color='black')\n",
    "    plt.title(method, fontsize=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./MountainCar/MountainCar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# labyrinth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "STEPS = [LEFT, DOWN, RIGHT, UP]\n",
    "OPPOSITE = {LEFT: RIGHT, RIGHT: LEFT, UP: DOWN, DOWN: UP}\n",
    "\n",
    "\n",
    "def step_grid(cur, d, size):\n",
    "    x, y = cur\n",
    "    if d == LEFT:\n",
    "        x -= 1\n",
    "    elif d == RIGHT:\n",
    "        x += 1\n",
    "    elif d == UP:\n",
    "        y -= 1\n",
    "    elif d == DOWN:\n",
    "        y += 1\n",
    "    if x < 0 or y < 0 or x >= size or y >= size:\n",
    "        return cur\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "def gen_labyrinth(size, np_random):\n",
    "    edges = np.zeros((size, size, 4), dtype=bool)\n",
    "    visit = np.zeros((size, size), dtype=bool)\n",
    "    stack = [(0, 0)]\n",
    "\n",
    "    while len(stack):\n",
    "        cur = stack.pop()\n",
    "        visit[cur] = 1\n",
    "        neib = [d for d in STEPS if not visit[step_grid(cur, d, size)]]\n",
    "        if len(neib):\n",
    "            stack.append(cur)\n",
    "            next_d = np_random.choice(neib)\n",
    "            next_pos = step_grid(cur, next_d, size)\n",
    "            edges[cur][next_d] = edges[next_pos][OPPOSITE[next_d]] = 1\n",
    "            stack.append(next_pos)\n",
    "    return edges\n",
    "\n",
    "\n",
    "class PolEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, size, max_steps):\n",
    "        self.size = size\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Discrete(4)\n",
    "        self.seed()\n",
    "        self.max_steps = max_steps\n",
    "        self.steps = 0\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in STEPS\n",
    "        if self.map[self.pos][action]:\n",
    "            self.pos = step_grid(self.pos, action, self.size)\n",
    "        self.visit[self.pos] = 1\n",
    "        self.steps += 1\n",
    "        done = self.visit.all() or self.steps >= self.max_steps\n",
    "        state = self.map[self.pos].astype(np.uint8)\n",
    "        reward = -1.0\n",
    "        return self.pos, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.map = gen_labyrinth(self.size, self.np_random)\n",
    "        self.visit = np.zeros((self.size, self.size), dtype=bool)\n",
    "        self.pos = (0,0)\n",
    "        self.visit[self.pos] = 1\n",
    "        self.steps = 0\n",
    "        return self.pos\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        m2 = np.zeros((self.size * 2 + 1, self.size * 2 + 1), dtype=int)\n",
    "        m2[1::2, 1::2] = 1\n",
    "        m2[:-1:2, 1::2] = self.map[:, :, LEFT]\n",
    "        m2[1::2, :-1:2] = self.map[:, :, UP]\n",
    "        m2[self.pos[0] * 2 + 1, self.pos[1] * 2 + 1] = 2\n",
    "\n",
    "        for s in m2.astype(str):\n",
    "            s = \"\".join(s)\n",
    "            s = s.replace(\"0\", \"#\")\n",
    "            s = s.replace(\"1\", \" \")\n",
    "            s = s.replace(\"2\", \"@\")\n",
    "            sys.stdout.write(s + \"\\n\")\n",
    "\n",
    "    def close(self):\n",
    "        self.map = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' For first-visit stats'''\n",
    "first_visits = {'ϵ-greedy':[], 'ϵz-greedy':[], 'DAE(ours)':[]}\n",
    "\n",
    "hidden_size = 512\n",
    "lr = 1e-4\n",
    "NUM_EPISODE = 50\n",
    "maze_size = 5\n",
    "max_steps = 700\n",
    "mu = 2\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "env = PolEnv(size=maze_size, max_steps=max_steps)\n",
    "action_space = 4\n",
    "obs_space = 2\n",
    "for method in ['ϵ-greedy', 'ϵz-greedy', 'DAE(ours)']:\n",
    "    for e in range(NUM_EPISODE):\n",
    "        print(f'epoch {e+1}/{NUM_EPISODE}', end = '    \\r')\n",
    "\n",
    "        explorer = Explorer(obs_space, action_space, hidden_size).to(device)\n",
    "        agent = Agent(explorer, lr)\n",
    "\n",
    "        state = env.reset()\n",
    "        first_visit = np.full((maze_size, maze_size), max_steps)\n",
    "        total_step = 1\n",
    "        duration = 0\n",
    "        while True:\n",
    "            ''' record first visit '''\n",
    "            row, col = state\n",
    "            if first_visit[row][col] == max_steps:\n",
    "                first_visit[row][col] = total_step\n",
    "            ''' end recording '''\n",
    "\n",
    "            if method == 'ϵ-greedy':\n",
    "                action = env.action_space.sample()\n",
    "            if method == 'ϵz-greedy':\n",
    "                if duration == 0:\n",
    "                    action   = env.action_space.sample()\n",
    "                    duration = np.random.zipf(mu, 1)\n",
    "                duration -= 1\n",
    "\n",
    "\n",
    "\n",
    "            if method == 'DAE(ours)':\n",
    "                mean = 0\n",
    "                std  = maze_size\n",
    "                state = ((row-mean)/std, (col-mean)/std)\n",
    "                action, log_prob = agent.sample(torch.FloatTensor(state).to(device))\n",
    "                agent.learn(log_prob)\n",
    "\n",
    "            next_state, _, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_step += 1\n",
    "            if done:\n",
    "                break\n",
    "        first_visits[method].append(first_visit)\n",
    "\n",
    "    first_visits[method] = np.array(first_visits[method])\n",
    "    first_visits[method] = first_visits[method].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "for i, method in enumerate(['ϵ-greedy', 'ϵz-greedy', 'DAE(ours)']):\n",
    "    fig.add_subplot(1,3,i+1)\n",
    "    plt.contour (first_visits[method], levels=6, vmin=0, vmax=max_steps ,cmap='jet', linewidths=2)\n",
    "    plt.contourf(first_visits[method], levels=6, vmin=0, vmax=max_steps ,cmap='jet', alpha=0.6)\n",
    "    plt.title(method, fontsize=24)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./Labyrinth/Labyrinth.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minigrid - multi room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gym_minigrid.wrappers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' For first-visit stats'''\n",
    "first_visits = {'ϵ-greedy':[], 'ϵz-greedy':[], 'DAE(ours)':[]}\n",
    "\n",
    "hidden_size = 512\n",
    "lr = 1e-4\n",
    "NUM_EPISODE = 10\n",
    "max_steps = 20000\n",
    "mu = 2\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "env = gym.make('MiniGrid-MultiRoom-N6-v0')\n",
    "env.max_steps = max_steps\n",
    "height = env.height\n",
    "width  = env.width\n",
    "action_space = env.action_space.n\n",
    "obs_space = 2\n",
    "\n",
    "for method in ['ϵ-greedy', 'ϵz-greedy', 'DAE(ours)']:\n",
    "    for e in range(NUM_EPISODE):\n",
    "        print(f'    epoch {e+1}/{NUM_EPISODE}', end = '    \\r')\n",
    "\n",
    "        explorer = Explorer(obs_space, action_space, hidden_size).to(device)\n",
    "        agent = Agent(explorer, lr)\n",
    "        env.seed(1)\n",
    "        env.reset()\n",
    "        first_visit = np.full((height, width), max_steps)\n",
    "        total_step = 1\n",
    "        duration = 0\n",
    "        while True:\n",
    "            ''' record first visit '''\n",
    "            col, row = env.agent_pos\n",
    "            if first_visit[row][col] == max_steps:\n",
    "                first_visit[row][col] = total_step\n",
    "            ''' end recording '''\n",
    "\n",
    "            if method == 'ϵ-greedy':\n",
    "                action = env.action_space.sample()\n",
    "            if method == 'ϵz-greedy':\n",
    "                if duration == 0:\n",
    "                    action   = env.action_space.sample()\n",
    "                    duration = np.random.zipf(mu, 1)\n",
    "                duration -= 1\n",
    "            if method == 'DAE(ours)':\n",
    "                mean = 0\n",
    "                std  = width\n",
    "                state = ((col-mean)/std, (row-mean)/std)\n",
    "                action, log_prob = agent.sample(torch.FloatTensor(state).to(device))\n",
    "                agent.learn(log_prob)\n",
    "\n",
    "            _, _, done, _ = env.step(action)\n",
    "            total_step += 1\n",
    "            if done:\n",
    "                break\n",
    "        first_visits[method].append(first_visit)\n",
    "\n",
    "    first_visits[method] = np.array(first_visits[method])\n",
    "    first_visits[method] = first_visits[method].mean(0)\n",
    "    first_visits[method] = np.flip(first_visits[method], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,3))\n",
    "for i, method in enumerate(['ϵ-greedy', 'ϵz-greedy', 'DAE(ours)']):\n",
    "    fig.add_subplot(1,3,i+1)\n",
    "    plt.contour (first_visits[method], vmin=0, vmax=max_steps ,cmap='jet', linewidths=2)\n",
    "    plt.contourf(first_visits[method], vmin=0, vmax=max_steps ,cmap='jet', alpha=0.6)\n",
    "    plt.title(method, fontsize=24)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./MultiRoom/MultiRoom.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw12_reinforcement_learning_chinese_version.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/ga642381/ML2021-Spring/blob/main/HW12/HW12_ZH.ipynb",
     "timestamp": 1624264979931
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
